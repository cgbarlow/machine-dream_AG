# Spec 13: LLM Connection Profile Management

**Status**: Draft
**Version**: 1.1
**Last Updated**: 2026-01-22

---

## Related ADRs

| ADR | Relationship |
|-----|--------------|
| [ADR-002: Local LLM Provider](../adr/002-local-llm-provider.md) | Authorizes LLM integration |

---

## 1. Overview

### 1.1 Purpose
Implement a comprehensive profile management system for LLM connections, allowing users to save, switch between, and manage multiple LLM provider configurations without manual editing of config files.

### 1.2 Motivation
- Users need to test multiple LLM providers (LM Studio, OpenAI, Anthropic, Ollama)
- Switching between local and cloud models is cumbersome
- API keys and URLs should be stored securely
- Different models require different configurations (temperature, max_tokens, etc.)
- Team collaboration requires sharing profiles

### 1.3 Success Criteria
- âœ… Save unlimited LLM provider profiles
- âœ… Switch active profile with single command
- âœ… Secure API key storage
- âœ… Profile validation and health checks
- âœ… Import/export profiles for sharing
- âœ… CLI-first with full TUI support

---

## 2. Profile Structure

### 2.1 Profile Schema

```typescript
interface LLMProfile {
  // Identity
  name: string;                      // Unique identifier (e.g., "lm-studio-local")
  description?: string;              // Human-readable description

  // Provider Configuration
  provider: LLMProvider;             // Provider type
  baseUrl: string;                   // API endpoint
  apiKey?: string;                   // API key (optional, stored securely)

  // Model Configuration
  model: string;                     // Model name/ID (friendly name for display)
  modelPath?: string;                // Full model path for LM Studio CLI (e.g., "Qwen/QwQ-32B-GGUF/qwq-32b-q8_0.gguf")
  llamaServerPath?: string;          // Path to llama-server executable (for llama-server provider)
  launchCommand?: string;            // Override launch command (for llama-server provider) - if set, used instead of auto-generated
  parameters: ModelParameters;       // Generation parameters

  // Metadata
  createdAt: number;                 // Unix timestamp
  modifiedAt?: number;               // Last modification timestamp
  lastUsed?: number;                 // Last usage timestamp
  usageCount: number;                // Number of times used
  isDefault: boolean;                // Is this the active profile?

  // Connection
  timeout: number;                   // Request timeout (ms)
  retries: number;                   // Max retry attempts

  // Tags & Organization
  tags: string[];                    // User-defined tags
  color?: string;                    // Display color for TUI

  // Custom Prompting
  systemPrompt?: string;             // Additional system prompt text (appended to base prompt)

  // Instance Support (parameter variants)
  instances?: Record<string, ProfileInstance>; // Instance name -> instance config
  activeInstance?: string;           // Currently active instance name (default: "default")
}

/**
 * Profile instance - a named configuration variant of a profile
 * Allows multiple parameter sets for the same profile/model
 */
interface ProfileInstance {
  name: string;                      // Instance identifier (e.g., "default", "20260122_test1")
  description?: string;              // Human-readable description
  parameters: ModelParameters;       // Generation parameters for this instance
  launchCommand?: string;            // Override launch command (for llama-server provider)
  createdAt: number;                 // Unix timestamp (milliseconds)
  modifiedAt?: number;               // Last modification timestamp
  lastUsed?: number;                 // Last usage timestamp
}

type LLMProvider =
  | 'lmstudio'      // LM Studio local server
  | 'llama-server'  // llama.cpp llama-server (direct)
  | 'openai'        // OpenAI API
  | 'anthropic'     // Anthropic API
  | 'ollama'        // Ollama local
  | 'openrouter'    // OpenRouter
  | 'custom';       // Custom OpenAI-compatible API

interface ModelParameters {
  temperature: number;               // 0.0 - 2.0 (default: 0.7)
  maxTokens: number;                 // Max response tokens
  topP?: number;                     // Nucleus sampling (0.0-1.0)
  topK?: number;                     // Top-K sampling (e.g., 50)
  minP?: number;                     // Min-P sampling (0.0-1.0, e.g., 0.01)
  frequencyPenalty?: number;         // Repetition penalty (-2.0 to 2.0)
  presencePenalty?: number;          // Topic diversity (-2.0 to 2.0)
  repeatPenalty?: number;            // Repeat penalty (1.0 = disabled)
  stop?: string[];                   // Stop sequences

  // DRY (Don't Repeat Yourself) sampling parameters
  dryMultiplier?: number;            // DRY penalty multiplier (e.g., 1.1)
  dryBase?: number;                  // DRY base value (e.g., 1.75)
  dryAllowedLength?: number;         // Min sequence length for DRY (e.g., 2)
  dryPenaltyLastN?: number;          // Context for DRY penalty (-1 = full context)
}
```

### 2.2 Storage Location

**Profile Storage:**
```
~/.machine-dream/llm-profiles.json
```

**Secure Key Storage (optional):**
```
~/.machine-dream/secrets/api-keys.enc
```

**Format:**
```json
{
  "version": "1.0",
  "profiles": {
    "lm-studio-local": {
      "name": "lm-studio-local",
      "description": "Local LM Studio with Qwen3 30B",
      "provider": "lmstudio",
      "baseUrl": "http://localhost:1234/v1",
      "model": "qwen3-30b",
      "parameters": {
        "temperature": 0.7,
        "maxTokens": 2048
      },
      "createdAt": 1704556800000,
      "usageCount": 42,
      "isDefault": true,
      "timeout": 60000,
      "retries": 3,
      "tags": ["local", "default"]
    },
    "openai-gpt4": {
      "name": "openai-gpt4",
      "description": "OpenAI GPT-4 Turbo",
      "provider": "openai",
      "baseUrl": "https://api.openai.com/v1",
      "apiKey": "${OPENAI_API_KEY}",
      "model": "gpt-4-turbo-preview",
      "parameters": {
        "temperature": 0.5,
        "maxTokens": 4096
      },
      "createdAt": 1704556800000,
      "usageCount": 15,
      "isDefault": false,
      "timeout": 120000,
      "retries": 5,
      "tags": ["cloud", "expensive"]
    },
    "glm-4-llama": {
      "name": "glm-4-llama",
      "description": "GLM-4.7 Flash via llama-server (auto-generated command)",
      "provider": "llama-server",
      "baseUrl": "http://127.0.0.1:8080",
      "model": "glm-4.7-flash",
      "modelPath": "C:\\Users\\user\\.lmstudio\\models\\unsloth\\GLM-4.7-Flash-GGUF\\GLM-4.7-Flash-UD-Q8_K_XL.gguf",
      "llamaServerPath": "C:\\Users\\user\\llama.cpp\\llama-server.exe",
      "parameters": {
        "temperature": 0.2,
        "maxTokens": 8192,
        "topK": 50,
        "topP": 0.95,
        "minP": 0.01,
        "dryMultiplier": 1.1
      },
      "createdAt": 1704556800000,
      "usageCount": 0,
      "isDefault": false,
      "timeout": 600000,
      "retries": 3,
      "tags": ["local", "llama-server", "moe"]
    },
    "glm-4-llama-custom": {
      "name": "glm-4-llama-custom",
      "description": "GLM-4.7 Flash via llama-server (custom launch command override)",
      "provider": "llama-server",
      "baseUrl": "http://127.0.0.1:8080",
      "model": "glm-4.7-flash",
      "modelPath": "C:\\Users\\user\\.lmstudio\\models\\unsloth\\GLM-4.7-Flash-GGUF\\GLM-4.7-Flash-UD-Q8_K_XL.gguf",
      "llamaServerPath": "C:\\Users\\user\\llama.cpp\\llama-server.exe",
      "launchCommand": "llama-server.exe --model \"C:\\Users\\user\\.lmstudio\\models\\unsloth\\GLM-4.7-Flash-GGUF\\GLM-4.7-Flash-UD-Q8_K_XL.gguf\" --port 8080 --ctx-size 16384 --n-gpu-layers 999 --flash-attn on",
      "parameters": {
        "temperature": 0.6,
        "maxTokens": 8192
      },
      "createdAt": 1704556800000,
      "usageCount": 0,
      "isDefault": false,
      "timeout": 600000,
      "retries": 3,
      "tags": ["local", "llama-server", "moe", "custom-cmd"]
    }
  },
  "activeProfile": "lm-studio-local"
}
```

---

## 3. Core Operations

### 3.1 Profile Management

**Create Profile:**
```typescript
profileManager.create({
  name: 'anthropic-claude',
  provider: 'anthropic',
  baseUrl: 'https://api.anthropic.com/v1',
  apiKey: process.env.ANTHROPIC_API_KEY,
  model: 'claude-3-5-sonnet-20241022',
  parameters: { temperature: 0.7, maxTokens: 4096 }
});
```

**List Profiles:**
```typescript
const profiles = profileManager.list({
  filter?: string,        // Filter by name/description
  tags?: string[],        // Filter by tags
  provider?: LLMProvider, // Filter by provider
  sortBy?: 'name' | 'lastUsed' | 'usageCount'
});
```

**Get Profile:**
```typescript
const profile = profileManager.get('lm-studio-local');
```

**Update Profile:**
```typescript
profileManager.update('lm-studio-local', {
  model: 'qwen3-32b',
  parameters: { temperature: 0.8 }
});
```

**Delete Profile:**
```typescript
profileManager.delete('old-profile');
```

**Set Active:**
```typescript
profileManager.setActive('anthropic-claude');
```

### 3.2 Profile Validation

**Health Check:**
```typescript
const health = await profileManager.healthCheck('lm-studio-local');
// Returns:
{
  isHealthy: boolean,
  responseTime: number,
  error?: string,
  modelInfo?: {
    name: string,
    contextLength: number,
    capabilities: string[]
  }
}
```

**Validate Configuration:**
```typescript
const validation = profileManager.validate(profile);
// Returns:
{
  isValid: boolean,
  errors: string[],
  warnings: string[]
}
```

### 3.3 Import/Export

**Export Profiles:**
```typescript
// Export all profiles (excluding API keys)
const exported = profileManager.export({
  includeSecrets: false,  // Don't export API keys
  profiles?: string[]      // Specific profiles (or all)
});

// Format:
{
  version: "1.0",
  exportedAt: 1704556800000,
  profiles: [...],
  metadata: {
    source: "Machine Dream v0.1.0",
    count: 5
  }
}
```

**Import Profiles:**
```typescript
profileManager.import(exportedData, {
  overwrite: false,        // Overwrite existing profiles
  skipInvalid: true,       // Skip invalid profiles
  setActive?: string       // Set specific profile as active
});
```

---

## 4. CLI Commands

### 4.1 Command Structure

```
machine-dream llm profile <command> [options]
```

### 4.2 Commands

#### `profile list`
```bash
# List all profiles
machine-dream llm profile list

# List with filter
machine-dream llm profile list --filter local

# List by provider
machine-dream llm profile list --provider lmstudio

# List by tags
machine-dream llm profile list --tags local,default

# Output formats
machine-dream llm profile list --format table    # Default
machine-dream llm profile list --format json
machine-dream llm profile list --format yaml
```

**Output (table format):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Name             â”‚ Provider â”‚ Model                â”‚ Status       â”‚ Used    â”‚ Active â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ lm-studio-local  â”‚ lmstudio â”‚ qwen3-30b            â”‚ âœ“ Healthy    â”‚ 42 uses â”‚ âœ“      â”‚
â”‚ openai-gpt4      â”‚ openai   â”‚ gpt-4-turbo-preview  â”‚ ? Untested   â”‚ 15 uses â”‚        â”‚
â”‚ anthropic-claude â”‚ anthropicâ”‚ claude-3-5-sonnet    â”‚ âœ— Offline    â”‚  0 uses â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### `profile add`
```bash
# Interactive mode (prompts for all fields)
machine-dream llm profile add

# With options
machine-dream llm profile add lm-studio-local \
  --provider lmstudio \
  --url http://localhost:1234/v1 \
  --model qwen3-30b \
  --temperature 0.7 \
  --max-tokens 2048 \
  --description "Local LM Studio"

# From template
machine-dream llm profile add my-openai --template openai-gpt4 \
  --api-key $OPENAI_API_KEY

# With tags
machine-dream llm profile add test-profile --tags local,testing,experimental

# llama-server with auto-generated launch command
machine-dream llm profile add glm-4-flash \
  --provider llama-server \
  --url http://127.0.0.1:8080 \
  --model glm-4.7-flash \
  --model-path "C:\Users\user\.lmstudio\models\unsloth\GLM-4.7-Flash-GGUF\GLM-4.7-Flash-UD-Q8_K_XL.gguf" \
  --llama-server-path "C:\Users\user\llama.cpp\llama-server.exe" \
  --temperature 0.4 --top-p 0.95 --min-p 0.01

# llama-server with custom launch command override
machine-dream llm profile add glm-4-flash-custom \
  --provider llama-server \
  --url http://127.0.0.1:8080 \
  --model glm-4.7-flash \
  --launch-command "llama-server.exe -m model.gguf --port 8080 --ctx-size 32768 --flash-attn"

# With custom system prompt (appended to base prompt)
machine-dream llm profile add my-profile \
  --provider lmstudio \
  --url http://localhost:1234/v1 \
  --model qwen3-30b \
  --system-prompt "Always explain your reasoning step by step."
```

#### `profile show`
```bash
# Show profile details
machine-dream llm profile show lm-studio-local

# Show with credentials (requires confirmation)
machine-dream llm profile show lm-studio-local --show-secrets
```

**Output:**
```
Profile: lm-studio-local
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Provider:     lmstudio
Base URL:     http://localhost:1234/v1
Model:        qwen3-30b
Temperature:  0.7
Max Tokens:   2048
Timeout:      60s
Retries:      3

Status:       âœ“ Healthy (responded in 245ms)
Created:      2024-01-06 10:30:00
Last Used:    2024-01-06 14:22:15
Usage Count:  42 times
Active:       Yes (default profile)

Tags:         local, default
```

#### `profile edit`
```bash
# Interactive edit
machine-dream llm profile edit lm-studio-local

# Update specific fields
machine-dream llm profile edit lm-studio-local \
  --model qwen3-32b \
  --temperature 0.8

# Add/remove tags
machine-dream llm profile edit lm-studio-local --add-tags production
machine-dream llm profile edit lm-studio-local --remove-tags testing
```

#### `profile delete`
```bash
# Delete with confirmation
machine-dream llm profile delete old-profile

# Force delete (no confirmation)
machine-dream llm profile delete old-profile --force

# Delete multiple
machine-dream llm profile delete profile1 profile2 profile3
```

#### `profile set`
```bash
# Set active profile
machine-dream llm profile set anthropic-claude

# Verify it's active
machine-dream llm profile show --active
```

#### `profile test`
```bash
# Test specific profile
machine-dream llm profile test lm-studio-local

# Test all profiles
machine-dream llm profile test --all

# Test with verbose output
machine-dream llm profile test lm-studio-local --verbose
```

**Output:**
```
Testing profile: lm-studio-local
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ“ Connection successful
âœ“ Authentication valid
âœ“ Model available: qwen3-30b
âœ“ Test request completed

Response Time: 245ms
Model Info:
  - Context Length: 32768 tokens
  - Supports streaming: Yes
  - Supports function calling: Yes

Status: Healthy âœ“
```

#### `profile export`
```bash
# Export all profiles
machine-dream llm profile export profiles-backup.json

# Export specific profiles
machine-dream llm profile export my-profiles.json --profiles lm-studio-local,openai-gpt4

# Export with secrets (requires confirmation)
machine-dream llm profile export full-backup.json --include-secrets

# Export as YAML
machine-dream llm profile export profiles.yaml --format yaml
```

#### `profile import`
```bash
# Import profiles
machine-dream llm profile import profiles-backup.json

# Import with overwrite
machine-dream llm profile import profiles-backup.json --overwrite

# Import and set active
machine-dream llm profile import profiles-backup.json --set-active lm-studio-local

# Dry run (validate without importing)
machine-dream llm profile import profiles-backup.json --dry-run
```

#### `profile clone`
```bash
# Clone a profile
machine-dream llm profile clone glm-4-flash glm-4-flash-v2

# Clone with custom description
machine-dream llm profile clone glm-4-flash glm-4-flash-experimental \
  --description "Experimental settings for higher creativity"

# Clone and set as active
machine-dream llm profile clone glm-4-flash glm-4-flash-prod --set-active
```

**Output:**
```
âœ… Profile cloned: glm-4-flash â†’ glm-4-flash-v2
   Description: Clone of glm-4-flash
   Provider: llama-server
   Model: glm-4-9b-chat-q8_0
```

#### `profile instance` - Manage Profile Instances

Instances allow multiple parameter configurations for the same profile/model.
Each profile starts with a `default` instance. The active instance determines
which parameters are used when the profile is loaded.

```bash
# List all instances for a profile
machine-dream llm profile instance list glm-4-flash

# Create new instance (copies from default or specified source)
machine-dream llm profile instance create glm-4-flash 20260122_test1 \
  --description "Test instance with temp 0.4" \
  --temperature 0.4 \
  --max-tokens 8192 \
  --top-p 0.95 \
  --min-p 0.01

# Create instance by copying from another
machine-dream llm profile instance create glm-4-flash high-temp \
  --copy-from default \
  --temperature 1.0

# Show instance details
machine-dream llm profile instance show glm-4-flash 20260122_test1

# Set active instance
machine-dream llm profile instance use glm-4-flash 20260122_test1

# Update instance parameters
machine-dream llm profile instance update glm-4-flash 20260122_test1 \
  --temperature 0.5 \
  --description "Updated test instance"

# Rename instance
machine-dream llm profile instance rename glm-4-flash 20260122_test1 production

# Delete instance (cannot delete 'default')
machine-dream llm profile instance delete glm-4-flash old-instance

# Clone instance
machine-dream llm profile instance clone glm-4-flash default experimental \
  --description "Experimental high-temp settings"

# Reset instance to default parameters
machine-dream llm profile instance reset glm-4-flash experimental

# Create instance with custom launch command (llama-server only)
machine-dream llm profile instance create glm-4-flash high-context \
  --launch-command "llama-server.exe -m model.gguf --ctx-size 32768 --flash-attn"
```

**Output (list):**
```
ğŸ”§ Instances for profile: glm-4-flash

â–¶ default (active)
   Description: Default configuration
   Temperature: 0.2 | Max Tokens: 8192
   Top P: 0.95
   Min P: 0.01
   Created: 1/22/2025, 5:00:00 AM

  20260122_test1
   Description: Test instance with temp 0.4
   Temperature: 0.4 | Max Tokens: 8192
   Top P: 0.95
   Min P: 0.01
   Created: 1/22/2026, 9:04:08 PM
```

---

## 5. TUI Screen

### 5.1 Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”Œ LLM Connection Profiles                                  [Add] [Import] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€ Active Profile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  âœ“ lm-studio-local                        ğŸŸ¢ Healthy (245ms)        â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  Provider: LM Studio                Model: qwen3-30b                â”‚  â”‚
â”‚  â”‚  URL: http://localhost:1234/v1      Temp: 0.7  Max: 2048           â”‚  â”‚
â”‚  â”‚  Last Used: 2 minutes ago           Used: 42 times                  â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  [Test] [Edit] [Switch]                              Tags: local    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€ All Profiles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  â–¶ lm-studio-local    lmstudio    qwen3-30b          ğŸŸ¢ âœ“ Active    â”‚  â”‚
â”‚  â”‚    openai-gpt4        openai      gpt-4-turbo        ğŸŸ¡ ? Untested  â”‚  â”‚
â”‚  â”‚    anthropic-claude   anthropic   claude-3-5-sonnet  ğŸ”´ âœ— Offline   â”‚  â”‚
â”‚  â”‚    ollama-local       ollama      llama3.1:70b       ğŸŸ¢ âœ“ Ready     â”‚  â”‚
â”‚  â”‚    custom-api         custom      custom-model       ğŸŸ¡ ? Unknown   â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  Showing 5 of 5 profiles                           Sort: Last Used  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€ Quick Actions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  [1] Test Selected        [4] Delete Selected                       â”‚  â”‚
â”‚  â”‚  [2] Edit Selected        [5] Test All                              â”‚  â”‚
â”‚  â”‚  [3] Set as Active        [6] Export All                            â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  Profiles: 5 | Active: 1 | Healthy: 2 | Offline: 1                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[â†‘â†“] Navigate [Enter] Details [1-6] Actions [A] Add [E] Edit [D] Delete [Q] Back
```

### 5.2 Add/Edit Profile Form

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”Œ Add LLM Profile                                          [Save] [Cancel] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€ Basic Information â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  â–¶ Profile Name:     [lm-studio-local________________]              â”‚  â”‚
â”‚  â”‚    Description:      [Local LM Studio with Qwen3 30B_]              â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    Provider:         [LM Studio          â–¼]                          â”‚  â”‚
â”‚  â”‚                      Options: LM Studio, OpenAI, Anthropic,          â”‚  â”‚
â”‚  â”‚                               Ollama, OpenRouter, Custom             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€ Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    Base URL:         [http://localhost:1234/v1_______]              â”‚  â”‚
â”‚  â”‚    API Key:          [â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢] (optional)  [ğŸ‘ Show]     â”‚  â”‚
â”‚  â”‚                      â„¹ Use ${ENV_VAR} for environment variables      â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    Timeout:          [60]s    Retries: [3]                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€ Model Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    Model:            [qwen3-30b______________]                       â”‚  â”‚
â”‚  â”‚    Temperature:      [0.7_] (0.0 - 2.0)   â”œâ”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”¤          â”‚  â”‚
â”‚  â”‚    Max Tokens:       [2048] (1 - 32768)   â”œâ”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    â˜ Top P sampling (advanced)                                      â”‚  â”‚
â”‚  â”‚    â˜ Frequency penalty                                               â”‚  â”‚
â”‚  â”‚    â˜ Presence penalty                                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€ Organization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    Tags:             [local, default_____________]                   â”‚  â”‚
â”‚  â”‚    Color:            [ğŸŸ¢ Green        â–¼]                             â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    â˜‘ Set as active profile                                          â”‚  â”‚
â”‚  â”‚    â˜‘ Test connection before saving                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  [  Save Profile  ]  [  Test Connection  ]  [  Cancel  ]                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[Tab] Next Field [Shift+Tab] Previous [Space] Toggle [Enter] Save [Esc] Cancel
```

---

## 6. Implementation Details

### 6.1 File Structure

```
src/
  llm/
    profiles/
      LLMProfileManager.ts         # Core manager class
      ProfileStorage.ts            # File I/O and persistence
      ProfileValidator.ts          # Validation logic
      ProfileEncryption.ts         # API key encryption (optional)
      types.ts                     # Type definitions
    config.ts                      # Updated with profile support
  cli/
    commands/
      llm-profile.ts               # CLI commands
  tui-ink/
    screens/
      ProfileManagerScreen.tsx     # TUI screen
      ProfileManagerScreen.interactive.tsx
    services/
      CLIExecutor.ts               # Add profile methods
```

### 6.2 llama-server Provider: Auto-Start Behavior

When using the `llama-server` provider, Machine Dream can automatically start the server if it's not running. The launch command is determined as follows:

**Priority:**
1. **If `launchCommand` is set**: Use the exact command provided (full override)
2. **If `llamaServerPath` + `modelPath` are set**: Auto-generate the launch command

**Auto-Generated Command Format:**
```bash
"<llamaServerPath>" -m "<modelPath>" --port <port> [sampling parameters from active instance]
```

**Included Sampling Parameters (when set in active instance):**
- `--temp <temperature>`
- `--top-p <topP>`
- `--top-k <topK>`
- `--min-p <minP>`
- `--repeat-penalty <repeatPenalty>`
- `--dry-multiplier <dryMultiplier>`
- `--dry-base <dryBase>`
- `--dry-allowed-length <dryAllowedLength>`
- `--dry-penalty-last-n <dryPenaltyLastN>`

**Path Separator Normalization:**
The auto-generator normalizes path separators to match the `llamaServerPath` format:
- If `llamaServerPath` uses backslashes (`\`), `modelPath` slashes are converted to backslashes
- If `llamaServerPath` uses forward slashes (`/`), `modelPath` backslashes are converted to forward slashes

**Example:**
```typescript
// Profile configuration
{
  provider: 'llama-server',
  baseUrl: 'http://127.0.0.1:8080',
  llamaServerPath: 'C:\\llama.cpp\\llama-server.exe',
  modelPath: 'models/qwen/qwen-32b-q8.gguf',
  parameters: {
    temperature: 0.4,
    topP: 0.95,
    minP: 0.01
  }
}

// Auto-generated command:
// "C:\llama.cpp\llama-server.exe" -m "models\qwen\qwen-32b-q8.gguf" --port 8080 --temp 0.4 --top-p 0.95 --min-p 0.01
```

**Per-Instance Launch Commands:**
Instances can override the launch command while keeping other parameters. This is useful for testing different startup configurations:

```bash
# Create instance with custom launch command
machine-dream llm profile instance create glm-4-flash high-context \
  --launch-command "llama-server.exe -m model.gguf --ctx-size 32768 --flash-attn"
```

### 6.3 Security Considerations

**API Key Storage:**
1. **Option 1: Environment Variables** (Recommended)
   - Store as `${OPENAI_API_KEY}` in profile
   - Resolve at runtime from environment
   - Never store plaintext in files

2. **Option 2: Encrypted Storage**
   - Use node's crypto module
   - Encrypt with machine-specific key
   - Decrypt at runtime
   - Still not 100% secure (keys on disk)

3. **Option 3: External Secret Management**
   - Support 1Password CLI
   - Support system keychain
   - Reference secrets by ID

**Implementation:**
```typescript
// Resolve API key at runtime
function resolveApiKey(key: string): string {
  if (key.startsWith('${') && key.endsWith('}')) {
    const envVar = key.slice(2, -1);
    return process.env[envVar] || '';
  }
  return key; // Use as-is (not recommended for production)
}
```

### 6.4 Backward Compatibility

**Migration from config.ts:**
```typescript
// On first run, create default profile from existing config
if (!fs.existsSync(profilesPath)) {
  const defaultProfile = {
    name: 'default',
    provider: 'lmstudio',
    baseUrl: DEFAULT_LLM_CONFIG.baseUrl,
    model: DEFAULT_LLM_CONFIG.model,
    parameters: {
      temperature: DEFAULT_LLM_CONFIG.temperature,
      maxTokens: DEFAULT_LLM_CONFIG.maxTokens
    },
    isDefault: true
  };
  profileManager.create(defaultProfile);
}
```

---

## 7. Testing Strategy

### 7.1 Unit Tests

```typescript
describe('LLMProfileManager', () => {
  it('creates profile with valid config', () => {});
  it('rejects profile with invalid name', () => {});
  it('updates existing profile', () => {});
  it('deletes profile safely', () => {});
  it('prevents deletion of active profile', () => {});
  it('sets active profile correctly', () => {});
  it('resolves environment variable API keys', () => {});
});
```

### 7.2 Integration Tests

```typescript
describe('Profile CLI Commands', () => {
  it('adds profile via CLI', () => {});
  it('lists profiles with correct formatting', () => {});
  it('exports and imports profiles', () => {});
  it('tests profile health check', () => {});
});
```

### 7.3 TUI Tests

```typescript
describe('ProfileManagerScreen', () => {
  it('renders profile list correctly', () => {});
  it('handles profile selection', () => {});
  it('validates form inputs', () => {});
  it('tests connection from UI', () => {});
});
```

---

## 8. Future Enhancements

### 8.1 Phase 2 Features
- **Profile Templates**: Pre-configured profiles for popular providers
- **Usage Analytics**: Track tokens, costs, and response times per profile
- **Auto-Detection**: Discover local LLM servers automatically
- **Model Discovery**: Query provider for available models
- **Prompt Templates**: Associate prompt templates with profiles
- **Cost Tracking**: Monitor API costs per profile
- **Rate Limiting**: Configure rate limits per profile

### 8.2 Advanced Features
- **Profile Groups**: Organize profiles into groups (dev, prod, testing)
- **Fallback Chains**: Automatic fallback to backup profiles
- **Load Balancing**: Distribute requests across multiple profiles
- **A/B Testing**: Compare model performance automatically
- **Cloud Sync**: Sync profiles across machines
- **Team Sharing**: Share profiles within organization

---

## 9. Success Metrics

- âœ… Users can switch between LLM providers in <5 seconds
- âœ… No manual config file editing required
- âœ… API keys never stored in plaintext
- âœ… Profile export/import works across machines
- âœ… Health checks complete in <2 seconds
- âœ… Zero downtime when switching profiles
- âœ… TUI provides intuitive profile management

---

## 10. References

- [Spec 11: LLM Integration](11-llm-integration.md)
- [Spec 09: CLI Architecture](09-cli-architecture.md)
- [Spec 10: Terminal UI](10-terminal-menu-interface-spec.md)
- OpenAI API Documentation
- Anthropic Claude API Documentation
- LM Studio API Documentation
